with this service we begin the Requirement 2 of Cadabra application:

Steps:

1.Perform data firehose exercise to send logs using kinesis agent into an EC2 to S3 bucket

2.Create an EC2 key pair if don't have one did before

3.Go to Service EMR
    -Create Cluster:
        General Configuration:
            -name: CadabraRecs
            -logging: True
            -s3 folder default
            -Launch mode: Cluster

        Software Configuration:
            -Applications: Spark

        Harduare configuration:
            -instance type: m5.xlarge
            -number of instances: 3
            -Ec2 key pair: The created previously
            -Permissions: default
        
        -Click on Create cluster to commit

4. Configure security group:
    -Into emr cluster summary
        -click on security groups for master
        -Edit inbound rules into the security group for master
            -Add rule:
                -Custom TCP IP
                -Port 22 -- MyIP

5. Connect:
    -Into emr cluster summary:
    -In Master public DNS: click on "SSH"
    -Take the host to connect and do an ssh connection

6. Into EMR Shell:
    -RUN "cp /usr/lib/spark/examples/src/main/python/ml/als_example.py ./"
    -RUN "nano als_example.py" ---->Inspect it and check how it works in a dfs and not lost a local system
    -RUN "hadoop fs -mkdir -p /user/hadoop/data/mllib/als"
    -RUN "hadoop fs -copyFromLocal /usr/lib/spark/data/mllib/als/sample_movielens_ratings.txt /user/hadoop/data/mllib/als/sample_movielens_ratings.txt"
        --->After perform the copy to hdfs, the entire cluster (core nodes and master) can access the data to proccess it
    -RUN "spark-submit als_example.py"

7. To check less logs and can see what really happens:
    -RUN "sudo nano als_example.py"
        -After the sparkSession is created insert:
            -"spark.sparkContext.setLogLevel("ERROR")"
    -RUN "spark-submit als_example.py"
    --->Don't matter really the results but they're are visible and can be seen the processing of the data there
    --->There's a recommender system that check movie comments and take the recommendations by Id

8. Now let's do the requirement 2:
    -Go to S3 bucket with logs generated by kinesis in the previous steps
    -Go to folder closets to data:
        -Select it
        -Actions:
            -Make public
            --->It's to make it accesible by EMR as public data (also can be used IAM role and policies to connect services)

    -Go to EMR sh
        -RUN "sudo nano als_example.py"
        -Go to "# $example on$" (after the previous log changes)
        -Remove the first 4 lines (lines, parts and ratingsRDD definition) "Ctrl+K"
        -Copy the "als-modifications.txt" content into this lines
        -Go forward at line on "als" variable definition and change the words "userId" --> "customerId"  "movieId" --> "itemId"
        -exit and save
        -RUN "spark-submit als_example.py"
        
        -->The results shows the recommended itemId to an specific customer ID
        -->if check logs in EC2 instance
            -> cd /var/log/cadabra
            -> grep 21899 *.log
            ----->There can check the item logs
                